{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mlfBRIFn2UuRjFLbgBRbUK046HQm4On4","timestamp":1727314134794},{"file_id":"1wqsYV8xRSBraHB1GC1lDtj0RxkuJhgdk","timestamp":1659476927673}],"toc_visible":true,"collapsed_sections":["9LHbOeeglQ3f","pOusbgzMdtnq"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["![header-notebooks.png](data:image/png;base64,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)\n","\n","Prática da **Aula 15: Machine Learning IV**, do curso de **Data Science** da **[Awari](https://awari.com.br/)**. Para utilizá-la, vá no menu \"Arquivo\" e, em seguida, na opção \"Salvar uma cópia no Drive\". Isto criará uma cópia deste notebook em uma pasta chamada \"Colab Notebooks\", no seu Google Drive pessoal. Use a cópia para criar novas células de código ou executar as células desta prática.\n","\n","---"],"metadata":{"id":"rPQopUmXNknp"}},{"cell_type":"markdown","source":["# **Computer Vision com CNN**"],"metadata":{"id":"q6wXv3vXh1EV"}},{"cell_type":"markdown","source":["## **Introdução**\n","\n","**Computer Vision** ou CV é a área de Deep Learning que se ocupa com a visão computacional, o reconhecimento de imagens, objetos ou pessoas e temas relacionados.\n","\n","**CNN**, por sua vez, é a sigla para *Convolutional Neural Network* (rede neural convolucional), um tipo de rede neural usado neste tipo de tarefa.\n","\n","Par um entendimento mais intuitivo sobre CNNs, recomendamos [este artigo](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) da comunidade Towards Data Science."],"metadata":{"id":"HDYiv3nx0HXy"}},{"cell_type":"markdown","source":["### **Ativando GPUs no Colab**\n","\n","Tarefas de CV normalmente demandam um alto poder computacional. Recomenda-se que seja acionado o uso de GPUs (Graphics Processing Units) aqui no Google Colab para uma perfomance melhor nesta prática.\n","\n","Isto pode ser feito por meio do menu \"Ambiente de Execução\", submenu \"Alterar o tipo de acelerador\", selecionando-se a opção \"GPU\"."],"metadata":{"id":"_VXQip42kkFX"}},{"cell_type":"markdown","source":["## **Começando a prática**\n","\n","Com isso, podemos começar a prática. O primeiro passo, como sempre, é importar as bibliotecas que usaremos (são várias)."],"metadata":{"id":"zL0Iw4DD1Rys"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXJr-nnfu83l"},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","import tarfile\n","from torchvision.datasets.utils import download_url\n","from torch.utils.data import random_split\n","!pip install jovian --upgrade -q\n","import jovian"]},{"cell_type":"code","source":["project_name='cnn_test'"],"metadata":{"id":"PlpGGSzYvMHW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n","download_url(dataset_url, '.')"],"metadata":{"id":"r9ySdzqzvPgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n","    tar.extractall(path='./data')"],"metadata":{"id":"93wRxWajvUGC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_dir = './data/cifar10'\n","\n","print(os.listdir(data_dir))\n","classes = os.listdir(data_dir + \"/train\")\n","print(classes)"],"metadata":{"id":"id3gYZ8dvUkx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["O conjunto de dados é extraído para o diretório data/cifar10. Ele contém 2 pastas (*train* e *test*, isto é, treino e teste, respectivamente), contendo o conjunto de treino (50.000 imagens) e o conjunto de teste (10.000 imagens), respectivamente. Cada uma delas contém 10 pastas, uma para cada classe de imagens."],"metadata":{"id":"v14D7KqEiZtU"}},{"cell_type":"code","source":["airplane_files = os.listdir(data_dir + \"/train/airplane\")\n","print('Número de exemplos de treinamento da classe:', len(airplane_files))\n","print(airplane_files[:5])"],"metadata":{"id":"IqpzzxCcvWA7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ship_files = os.listdir(data_dir + \"/test/ship\")\n","print(\"'Número de exemplos de treinamento da classe:\", len(ship_files))\n","print(ship_files[:5])"],"metadata":{"id":"Abbh1ELEvZkd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A estrutura de diretórios acima (uma pasta por classe) é usada por muitos conjuntos de dados de visão computacional, e a maioria das bibliotecas de aprendizado profundo fornece utilitários para trabalhar com esses conjuntos de dados. Podemos usar a *ImageFolderclasse* do torchvision para carregar os dados como tensores PyTorch."],"metadata":{"id":"lWcxMikii9DM"}},{"cell_type":"code","source":["from torchvision.datasets import ImageFolder\n","from torchvision.transforms import ToTensor"],"metadata":{"id":"SI8rFsWUvlgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = ImageFolder(data_dir+'/train', transform=ToTensor())"],"metadata":{"id":"_tn3GYcRvqJ2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vejamos um elemento de amostra do conjunto de dados de treinamento. Cada elemento é uma tupla, contendo um tensor de imagem e um rótulo. Como os dados consistem em imagens coloridas de 32x32 px com 3 canais (RGB), cada tensor de imagem tem a forma (3, 32, 32)."],"metadata":{"id":"Cck9YwDjjIbe"}},{"cell_type":"code","source":["img, label = dataset[0]\n","print(img.shape, label)\n","img\n","print(dataset.classes)"],"metadata":{"id":"U3QnhsDMvqf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","matplotlib.rcParams['figure.facecolor'] = '#ffffff'"],"metadata":{"id":"O-Df5MwevtvO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos visualizar a imagem usando matplotlib, mas precisamos alterar as dimensões do tensor para (32,32,3). Vamos criar uma função auxiliar para exibir uma imagem e seu rótulo."],"metadata":{"id":"06K8Fx2Fj03k"}},{"cell_type":"code","source":["def show_example(img, label):\n","    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n","    plt.imshow(img.permute(1, 2, 0))"],"metadata":{"id":"ZtTYXHhqvyt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_example(*dataset[0])"],"metadata":{"id":"P5SP6N_qv1Bv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_example(*dataset[1099])"],"metadata":{"id":"Yof7RwZ6v2wu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ao construir modelos de aprendizado de máquina do mundo real, é bastante comum dividir o conjunto de dados em 3 partes:\n","\n","1. **Conjunto de treinamento:** usado para treinar o modelo, ou seja, calcular a perda e ajustar os pesos do modelo usando gradiente descendente;\n","\n","2. **Conjunto de validação:** usado para avaliar o modelo durante o treinamento, ajustar hiperparâmetros (taxa de aprendizado etc.) e escolher a melhor versão do modelo;\n","\n","3. **Conjunto de teste:** usado para comparar diferentes modelos ou diferentes tipos de abordagens de modelagem e relatar a precisão final do modelo.\n","\n","Como não há um conjunto de validação predefinido, podemos separar uma pequena parte (5.000 imagens) do conjunto de treinamento para ser usado como conjunto de validação.\n","\n","Usaremos ```random_split``` para auxiliar a PyTorch fazer isso. Para garantir que sempre criamos o mesmo conjunto de validação, também definiremos uma semente para o gerador de números aleatórios."],"metadata":{"id":"wdoKJuLqkDIT"}},{"cell_type":"code","source":["# Treinamento e validação\n","random_seed = 42\n","torch.manual_seed(random_seed);\n","val_size = 5000\n","train_size = len(dataset) - val_size\n","\n","train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","len(train_ds), len(val_ds)"],"metadata":{"id":"aV8XUDMMv5Yi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A biblioteca ```jovian``` também fornece uma API simples para registrar parâmetros importantes relacionados ao conjunto de dados, treinamento do modelo, resultados etc. para fácil referência e comparação entre vários experimentos."],"metadata":{"id":"srk6_RpLkyIC"}},{"cell_type":"code","source":["  #jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"],"metadata":{"id":"yhfiI2dWwAoi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora podemos criar carregadores de dados para treinamento e validação, para carregar os dados em lotes\n","\n"],"metadata":{"id":"Hi4b8bVik8PM"}},{"cell_type":"code","source":["from torch.utils.data.dataloader import DataLoader\n","\n","batch_size=128\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"],"metadata":{"id":"l8gqbDTJwC6N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos observar lotes de imagens do conjunto de dados usando o método ```make_grid``` de torchvision.\n","\n","Cada vez que o código a seguir é executado, obtemos um bach diferente, pois o amostrador embaralha os índices antes de criar os lotes."],"metadata":{"id":"LiXqkGjJlEdg"}},{"cell_type":"code","source":["from torchvision.utils import make_grid\n","\n","def show_batch(dl):\n","    for images, labels in dl:\n","        fig, ax = plt.subplots(figsize=(12, 6))\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n","        break"],"metadata":{"id":"_pXj4LiDxmuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_batch(val_dl)"],"metadata":{"id":"XH7dRozFxqDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#jovian.commit(project=cnn_test, environment=None)"],"metadata":{"id":"Eea5TFb_lKDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Definindo o modelo**\n","\n","Usaremos uma rede neural convolucional por meio da classe ```nn.Conv2d``` do PyTorch.\n","\n","A convolução 2D é uma operação bastante simples: você começa com um *kernel*, que é simplesmente uma pequena matriz de pesos. Esse kernel \"desliza\" sobre os dados de entrada 2D, realizando uma multiplicação elemento a elemento com a parte da entrada em que está atualmente e, em seguida, somando os resultados em um único pixel de saída."],"metadata":{"id":"9LHbOeeglQ3f"}},{"cell_type":"code","source":["def kernel(image, kernel):\n","    ri, ci = image.shape       # Dimensão da imagem\n","    rk, ck = kernel.shape      # Dimensão do Kernel\n","    ro, co = ri-rk+1, ci-ck+1  # Dimensão de saída\n","    output = torch.zeros([ro, co])\n","    for i in range(ro):\n","        for j in range(co):\n","            output[i,j] = torch.sum(image[i:i+rk,j:j+ck] * kernel)\n","    return output"],"metadata":{"id":"Fyqgobwlxsul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_image = torch.tensor([\n","    [3, 3, 2, 1, 0],\n","    [0, 0, 1, 3, 1],\n","    [3, 1, 2, 2, 3],\n","    [2, 0, 0, 2, 2],\n","    [2, 0, 0, 0, 1]\n","], dtype=torch.float32)\n","\n","sample_kernel = torch.tensor([\n","    [0, 1, 2],\n","    [2, 2, 0],\n","    [0, 1, 2]\n","], dtype=torch.float32)\n","\n","kernel(sample_image, sample_kernel)"],"metadata":{"id":"LeJvDiXsx3hh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"FCAwBuvqx4np"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para imagens multicanal, um *kernel* diferente é aplicado a cada canal e as saídas são somadas em pixels.\n","\n","* **Menos parâmetros:** um pequeno conjunto de parâmetros (o *kernel*) é usado para calcular as saídas de toda a imagem, de modo que o modelo tem muito menos parâmetros em comparação com uma camada totalmente conectada;\n","* **Esparsidade de conexões:** em cada camada, cada elemento de saída depende apenas de um pequeno número de elementos de entrada, o que torna os passes para frente e para trás mais eficientes;\n","* **Compartilhamento de parâmetros e invariância espacial:** os recursos aprendidos por um kernel em uma parte da imagem podem ser usados ​​para detectar padrões semelhantes em uma parte diferente de outra imagem.\n","\n","Também usaremos camadas de *pool* máximo para diminuir progressivamente a altura e a largura dos tensores de saída de cada camada convolucional."],"metadata":{"id":"63CnKnIZl7tu"}},{"cell_type":"markdown","source":["Antes de definir todo o modelo, vamos ver como uma única camada convolucional seguida por uma camada *max-pooling* opera nos dados."],"metadata":{"id":"ObUO0a51mVaM"}},{"cell_type":"code","source":["simple_model = nn.Sequential(\n","    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n","    nn.MaxPool2d(2, 2)\n",")"],"metadata":{"id":"Y3WXwSrmyLID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = simple_model(images)\n","    print('out.shape:', out.shape)\n","    break"],"metadata":{"id":"UEtbte-4yLfh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A camada ```Conv2d``` transforma uma imagem de 3 canais em um mapa de recursos de 16 canais e a MaxPool2dcamada divide a altura e a largura pela metade.\n","\n","O mapa de recursos fica menor à medida que adicionamos mais camadas, até que finalmente ficamos com um pequeno mapa de recursos, que pode ser achatado em um vetor.\n","\n","Podemos então adicionar algumas camadas totalmente conectadas no final para obter um vetor de tamanho 10 para cada imagem."],"metadata":{"id":"fzktHkAamgTE"}},{"cell_type":"markdown","source":["Vamos definir o modelo estendendo uma `ImageClassificationBaseclasse`, que contém métodos auxiliares para treinamento e validação."],"metadata":{"id":"XBqsl68RnBTz"}},{"cell_type":"code","source":["class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                  # Gera predições\n","        loss = F.cross_entropy(out, labels) # Calcula perca\n","        return loss\n","\n","    def validation_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                    # Gera predições\n","        loss = F.cross_entropy(out, labels)   # Calcula Perca\n","        acc = accuracy(out, labels)           # Calcula Acurácia\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combina a perca entre as camadas\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combina a acurácia entre as camadas\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n","\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"],"metadata":{"id":"Ni7ePTvPyO0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Cifar10CnnModel(ImageClassificationBase):\n","    def __init__(self):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n","\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n","\n","            nn.Flatten(),\n","            nn.Linear(256*4*4, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10))\n","\n","    def forward(self, xb):\n","        return self.network(xb)"],"metadata":{"id":"wSHFv8p-orMB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Usaremos `nn.Sequential` para encadear as camadas e funções de ativação em uma única arquitetura de rede.\n","\n"],"metadata":{"id":"mryhM5flnfo9"}},{"cell_type":"code","source":["model = Cifar10CnnModel()\n","model"],"metadata":{"id":"V5nPmxmynex0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos verificar se o modelo produz a saída esperada em um lote de dados de treinamento.\n","\n","As 10 saídas para cada imagem podem ser interpretadas como probabilidades para as 10 classes alvo (após a aplicação do *softmax*), e a classe com maior probabilidade é escolhida como o rótulo previsto pelo modelo para a imagem de entrada."],"metadata":{"id":"0cp_9LaIpAHZ"}},{"cell_type":"code","source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = model(images)\n","    print('out.shape:', out.shape)\n","    print('out[0]:', out[0])\n","    break"],"metadata":{"id":"jXYgPDnuo7H_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gpu_validate():\n","    \"\"\"Ultiliza a GPU se disponível\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","def to_device(data, device):\n","    \"\"\"Move o tensor para o hardware escolhido\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Envolve os dados para envia-lo para o harware\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        \"\"\"Geração de batch e envio para o hardware (cpu/gpu)\"\"\"\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Número de batchs\"\"\"\n","        return len(self.dl)"],"metadata":{"id":"qrEjCIRCy-qT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Agora podemos agrupar nossos carregadores de dados de treinamento e validação usando `DeviceDataLoaderpara` transferir automaticamente lotes de dados para a GPU (se disponível).\n","\n","\n","\n"],"metadata":{"id":"Yu-LXRhyvPz4"}},{"cell_type":"code","source":["device = gpu_validate()\n","device"],"metadata":{"id":"EV51lChszCDk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Em seguida, podemos agrupar nossos carregadores de dados de treinamento e validação usando `DeviceDataLoader` para transferir automaticamente lotes de dados para a GPU (se disponível) e usar `to_device` para mover nosso modelo para a GPU (se disponível)."],"metadata":{"id":"hPnZZ9YYp-bc"}},{"cell_type":"code","source":["train_dl = DeviceDataLoader(train_dl, device)\n","val_dl = DeviceDataLoader(val_dl, device)\n","to_device(model, device);"],"metadata":{"id":"HTPpBjmgzF8H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos definir duas funções: `fit` e `evaluate`. Treinaremos o modelo usando gradiente descendente e avaliaremos seu desempenho no conjunto de validação."],"metadata":{"id":"ls2W5EWRqQbp"}},{"cell_type":"code","source":["def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n","    history = []\n","    optimizer = opt_func(model.parameters(), lr)\n","    for epoch in range(epochs):\n","        # Treinamento\n","        model.train()\n","        train_losses = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        # Validação\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    return history"],"metadata":{"id":"adw835egzVjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = to_device(Cifar10CnnModel(), device)\n","evaluate(model, val_dl)"],"metadata":{"id":"u3_dPovyzWPL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A precisão inicial é de cerca de 10%, que é o que se poderia esperar de um modelo inicializado aleatoriamente (já que ele tem uma chance de 1 em 10 de acertar um rótulo de forma aleatória).\n","\n","Usaremos os seguintes hiperparâmetros (taxa de aprendizado, número de épocas, *batch_size* etc.) para treinar nosso modelo:"],"metadata":{"id":"WeC3sef8q5H8"}},{"cell_type":"code","source":["#Instâncias do treinamento\n","#num_epochs = 10\n","num_epochs = 5\n","opt_func = torch.optim.Adam\n","lr = 0.001"],"metadata":{"id":"7zBEM212zqCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"],"metadata":{"id":"pFTagfJ6zxq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_accuracies(history):\n","    accuracies = [x['val_acc'] for x in history]\n","    plt.plot(accuracies, '-x')\n","    plt.xlabel('Época')\n","    plt.ylabel('Acurácia')\n","    plt.title('Acurácia vs. Número de épocas');\n","\n","plot_accuracies(history)"],"metadata":{"id":"_dR2TyiFz03J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_losses(history):\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('Época')\n","    plt.ylabel('Perda')\n","    plt.legend(['Treinamento', 'Validação'])\n","    plt.title('Perda vs. número de épocas');\n","plot_losses(history)"],"metadata":{"id":"N7XkezgJz7OO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Testando o modelo**\n","\n","Utilizando a primeira imagem do dataset:"],"metadata":{"id":"pOusbgzMdtnq"}},{"cell_type":"code","source":["def predict_image(img, model):\n","    # Convert to a batch of 1\n","    xb = to_device(img.unsqueeze(0), device)\n","    # Get predictions from model\n","    yb = model(xb)\n","    # Pick index with highest probability\n","    _, preds  = torch.max(yb, dim=1)\n","    # Retrieve the class label\n","    return dataset.classes[preds[0].item()]"],"metadata":{"id":"SU2EvsvGz9s_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos escolher outra imagem da amostra modificando o ponteiro do dataset:"],"metadata":{"id":"WeZVNWMMd0Zl"}},{"cell_type":"code","source":["test_dataset = ImageFolder(data_dir+'/test', transform=ToTensor())"],"metadata":{"id":"JMObrLb9ejnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img, label = test_dataset[5]\n","plt.imshow(img.permute(1, 2, 0))\n","print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))"],"metadata":{"id":"3ro8QTP-LRyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n","result = evaluate(model, test_loader)\n","result"],"metadata":{"id":"S-1mLk3XfgQO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Atingimos uma acurácia de 70% no treinamento. Para entender mais sobre métricas, recomendamos a leitura [deste artigo](https://analyticsindiamag.com/loss-functions-in-deep-learning-an-overview/#:~:text=Neural%20Network%20uses%20optimising%20strategies,bad%20the%20model%20is%20performing)."],"metadata":{"id":"0SW8dK3P5cVx"}},{"cell_type":"markdown","source":["---\n","\n","Notebook utilizado para fins educacionais da **Awari**.\n","\n","**© AWARI. Todos os direitos reservados.**"],"metadata":{"id":"lsqGTCZ2Nxfi"}}]}